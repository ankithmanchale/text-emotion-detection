{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from preprocessing import clean_text \n",
    "from model import build_model, train_model, evaluate_model, predict_emotion\n",
    "from utils import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import keras\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('text.csv')\n",
    "\n",
    "# Preprocess data\n",
    "df = df.drop('Number', axis=1)\n",
    "label_encoder = LabelEncoder()\n",
    "df['label_encoded'] = label_encoder.fit_transform(df['label'])\n",
    "# Manually create the label mapping\n",
    "label_mapping = dict(zip(label_encoder.inverse_transform(df['label_encoded']), df['label_encoded']))\n",
    "df=df.drop('label',axis=1)\n",
    "df=df.rename(columns={'label_encoded':'label'})\n",
    "\n",
    "unique_review = df['text'].unique()\n",
    "\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "df['cleaned_text'] = df['cleaned_text'].str.replace(\"http\", \"\").str.replace(\"href\", \"\").str.replace(\"img\", \"\").str.replace(\"irc\", \"\")\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_text'], df['label'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m9118/9118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3ms/step - accuracy: 0.8273 - loss: 0.4222 - val_accuracy: 0.9174 - val_loss: 0.1600\n",
      "Epoch 2/50\n",
      "\u001b[1m9118/9118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3ms/step - accuracy: 0.9238 - loss: 0.1487 - val_accuracy: 0.9177 - val_loss: 0.1587\n",
      "Epoch 3/50\n",
      "\u001b[1m9118/9118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3ms/step - accuracy: 0.9292 - loss: 0.1321 - val_accuracy: 0.9149 - val_loss: 0.1590\n",
      "Epoch 4/50\n",
      "\u001b[1m9118/9118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3ms/step - accuracy: 0.9341 - loss: 0.1191 - val_accuracy: 0.9143 - val_loss: 0.1663\n",
      "Epoch 5/50\n",
      "\u001b[1m9118/9118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3ms/step - accuracy: 0.9374 - loss: 0.1103 - val_accuracy: 0.9113 - val_loss: 0.1718\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=50000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_padded = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=100, padding='post')\n",
    "X_test_padded = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=100, padding='post')\n",
    "with open('tokenizer.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenizer, file)\n",
    "# Load the model architecture and weights\n",
    "\"\"\"json_file2 = open('model.json', 'r')\n",
    "loaded_model_json2 = json_file2.read()\n",
    "json_file2.close()\n",
    "loaded_model2 = model_from_json(loaded_model_json2)\n",
    "loaded_model2.load_weights(\"model.weights.h5\")\"\"\"\n",
    "mymodel = build_model(input_dim=50000, output_dim=len(label_encoder.classes_))\n",
    "myhistory = train_model(mymodel, X_train_padded, y_train, X_test_padded, y_test, epochs=50)\n",
    "model_json = mymodel.to_json()\n",
    "with open(\"model2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "mymodel.save_weights(\"model2.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file2 = open('model2.json', 'r')\n",
    "loaded_model_json2 = json_file2.read()\n",
    "json_file2.close()\n",
    "loaded_model2 = model_from_json(loaded_model_json2)\n",
    "loaded_model2.load_weights(\"model2.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3908/3908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 894us/step - accuracy: 0.9179 - loss: 0.1587\n",
      "Test Loss: 0.15868227183818817\n",
      "Test Accuracy: 0.9177243113517761\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "loss, accuracy = evaluate_model(loaded_model2, X_test_padded, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
